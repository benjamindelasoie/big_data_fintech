"""
CORRECTED Data Loading Pipeline for Fintech ETL

This module handles loading the CORRECTED transformed analytics data 
with proper baselines and realistic metrics.

CORRECTED Data Loading Strategy:
1. Load corrected user segments data (based on 13,000 users)
2. Load corrected comprehensive user metrics  
3. Create corrected aggregated funnel analysis tables
4. Generate corrected time-series analytics (monthly/weekly)
5. Store all with proper baselines and realistic percentages
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from config.spark_config import create_spark_session, get_cassandra_config
from config.etl_config import (
    PROCESSED_DATA_PATHS, 
    CASSANDRA_CONFIG,
    USER_SEGMENTS
)
from src.utils import (
    setup_logging, 
    read_parquet_file
)
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, lit, count, sum as spark_sum, avg, max as spark_max, min as spark_min,
    date_format, concat, desc, asc, round as spark_round
)
from pyspark.sql.types import DoubleType
import logging

class CorrectedDataLoader:
    """
    CORRECTED data loading and Cassandra integration class
    """
    
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.logger = setup_logging()
        self.cassandra_config = get_cassandra_config()
        
    def load_corrected_transformed_datasets(self):
        """
        Load all CORRECTED transformed datasets from parquet files
        """
        self.logger.info("Loading CORRECTED transformed datasets...")
        
        datasets = {}
        datasets["user_segments"] = read_parquet_file(self.spark, PROCESSED_DATA_PATHS["user_metrics"])
        datasets["comprehensive_metrics"] = read_parquet_file(self.spark, PROCESSED_DATA_PATHS["final_analytics"])
        
        # Log corrected counts
        segments_count = datasets["user_segments"].count()
        metrics_count = datasets["comprehensive_metrics"].count()
        
        self.logger.info(f"âœ… CORRECTED transformed datasets loaded:")
        self.logger.info(f"  - User segments: {segments_count:,} users with transactions")
        self.logger.info(f"  - Comprehensive metrics: {metrics_count:,} total users")
        
        return datasets
    
    def create_corrected_aggregated_funnel_analysis(self, comprehensive_metrics_df):
        """
        Create CORRECTED aggregated funnel analysis data with proper baselines
        """
        self.logger.info("Creating CORRECTED aggregated funnel analysis...")
        
        # Overall funnel analysis with corrected baseline
        overall_funnel = comprehensive_metrics_df.agg(
            count("*").alias("total_users"),
            spark_sum(when(col("drop_flag") == 0, 1).otherwise(0)).alias("stage_2_returned"),
            spark_sum("activation_validated").alias("stage_3_activated"),
            spark_sum("setup_achieved").alias("stage_4_setup"),
            spark_sum("habit_validated").alias("stage_5_habit")
        ).withColumn("dimension_type", lit("overall")
        ).withColumn("dimension_value", lit("all")
        ).withColumn("stage_1_registered", col("total_users"))
        
        # Funnel by user segment with corrected counts
        segment_funnel = comprehensive_metrics_df.groupBy("user_segment").agg(
            count("*").alias("total_users"),
            spark_sum(when(col("drop_flag") == 0, 1).otherwise(0)).alias("stage_2_returned"),
            spark_sum("activation_validated").alias("stage_3_activated"),
            spark_sum("setup_achieved").alias("stage_4_setup"),
            spark_sum("habit_validated").alias("stage_5_habit")
        ).withColumn("dimension_type", lit("user_segment")
        ).withColumn("dimension_value", col("user_segment")
        ).withColumn("stage_1_registered", col("total_users")
        ).drop("user_segment")
        
        # Funnel by first login month with corrected data
        month_funnel = comprehensive_metrics_df.groupBy("first_login_month").agg(
            count("*").alias("total_users"),
            spark_sum(when(col("drop_flag") == 0, 1).otherwise(0)).alias("stage_2_returned"),
            spark_sum("activation_validated").alias("stage_3_activated"),
            spark_sum("setup_achieved").alias("stage_4_setup"),
            spark_sum("habit_validated").alias("stage_5_habit")
        ).withColumn("dimension_type", lit("month")
        ).withColumn("dimension_value", col("first_login_month").cast("string")
        ).withColumn("stage_1_registered", col("total_users")
        ).drop("first_login_month")
        
        # Union all funnel analyses
        funnel_analysis = overall_funnel.unionByName(segment_funnel).unionByName(month_funnel)
        
        self.logger.info("âœ… CORRECTED aggregated funnel analysis created successfully")
        return funnel_analysis
    
    def create_corrected_monthly_metrics(self, comprehensive_metrics_df):
        """
        Create CORRECTED monthly aggregated metrics by user segment
        """
        self.logger.info("Creating CORRECTED monthly metrics...")
        
        monthly_metrics = comprehensive_metrics_df.groupBy("first_login_month", "user_segment").agg(
            count("*").alias("total_users"),
            spark_sum(when(col("drop_flag") == 0, 1).otherwise(0)).alias("returned_users"),
            spark_sum("activation_validated").alias("activated_users"),
            spark_sum("setup_achieved").alias("setup_completed_users"),
            spark_sum("habit_validated").alias("habit_achieved_users")
        ).withColumn("year_month", 
            concat(lit("2022-"), 
                   when(col("first_login_month") < 10, 
                        concat(lit("0"), col("first_login_month")))
                   .otherwise(col("first_login_month").cast("string")))
        ).withColumn("drop_rate",
            spark_round((lit(1.0) - col("returned_users").cast(DoubleType()) / col("total_users").cast(DoubleType())) * 100, 2)
        ).withColumn("activation_rate",
            spark_round(col("activated_users").cast(DoubleType()) / col("total_users").cast(DoubleType()) * 100, 2)
        ).withColumn("setup_rate",
            spark_round(col("setup_completed_users").cast(DoubleType()) / col("total_users").cast(DoubleType()) * 100, 2)
        ).withColumn("habit_rate",
            spark_round(col("habit_achieved_users").cast(DoubleType()) / col("total_users").cast(DoubleType()) * 100, 2)
        ).select(
            "year_month", "user_segment", "total_users", "returned_users", 
            "activated_users", "setup_completed_users", "habit_achieved_users",
            "drop_rate", "activation_rate", "setup_rate", "habit_rate"
        )
        
        self.logger.info("âœ… CORRECTED monthly metrics created successfully")
        return monthly_metrics
    
    def create_corrected_weekly_metrics(self, comprehensive_metrics_df):
        """
        Create CORRECTED weekly aggregated metrics by user segment
        """
        self.logger.info("Creating CORRECTED weekly metrics...")
        
        weekly_metrics = comprehensive_metrics_df.groupBy("first_login_week", "user_segment").agg(
            count("*").alias("total_users"),
            spark_sum(when(col("drop_flag") == 0, 1).otherwise(0)).alias("returned_users"),
            spark_sum("activation_validated").alias("activated_users"),
            spark_sum("setup_achieved").alias("setup_completed_users"),
            spark_sum("habit_validated").alias("habit_achieved_users")
        ).withColumn("year_week",
            concat(lit("2022-W"), 
                   when(col("first_login_week") < 10, 
                        concat(lit("0"), col("first_login_week")))
                   .otherwise(col("first_login_week").cast("string")))
        ).withColumn("drop_rate",
            spark_round((lit(1.0) - col("returned_users").cast(DoubleType()) / col("total_users").cast(DoubleType())) * 100, 2)
        ).withColumn("activation_rate",
            spark_round(col("activated_users").cast(DoubleType()) / col("total_users").cast(DoubleType()) * 100, 2)
        ).withColumn("setup_rate",
            spark_round(col("setup_completed_users").cast(DoubleType()) / col("total_users").cast(DoubleType()) * 100, 2)
        ).withColumn("habit_rate",
            spark_round(col("habit_achieved_users").cast(DoubleType()) / col("total_users").cast(DoubleType()) * 100, 2)
        ).select(
            "year_week", "user_segment", "total_users", "returned_users",
            "activated_users", "setup_completed_users", "habit_achieved_users", 
            "drop_rate", "activation_rate", "setup_rate", "habit_rate"
        )
        
        self.logger.info("âœ… CORRECTED weekly metrics created successfully")
        return weekly_metrics
    
    def write_to_storage_with_fallback(self, df, table_name, mode="overwrite"):
        """
        Write DataFrame to storage with Cassandra fallback mechanism
        """
        try:
            # Try Cassandra first
            df.write \
                .format("org.apache.spark.sql.cassandra") \
                .options(table=table_name, keyspace=CASSANDRA_CONFIG["keyspace"]) \
                .mode(mode) \
                .save()
            
            self.logger.info(f"âœ… Successfully wrote {df.count()} rows to Cassandra table: {table_name}")
            
        except Exception as e:
            self.logger.error(f"âš ï¸  Cassandra not available for table {table_name}: {e}")
            # Fallback to parquet with corrected prefix
            fallback_path = f"processed_data/corrected_cassandra_fallback_{table_name}.parquet"
            df.write.mode(mode).parquet(fallback_path)
            self.logger.info(f"âœ… Fallback: Saved corrected data to {fallback_path}")
    
    def load_corrected_user_segments_table(self, user_segments_df):
        """
        Load CORRECTED user segments data to storage
        """
        self.logger.info("Loading CORRECTED user segments table...")
        
        # Prepare corrected data for storage
        corrected_user_segments = user_segments_df.select(
            "user_id",
            "user_segment", 
            "total_transactions",
            "unique_transaction_types",
            "transaction_days",
            "first_transaction_dt",
            "last_transaction_dt"
        )
        
        self.write_to_storage_with_fallback(corrected_user_segments, "user_segments")
    
    def load_corrected_user_metrics_table(self, comprehensive_metrics_df):
        """
        Load CORRECTED comprehensive user metrics to storage
        """
        self.logger.info("Loading CORRECTED user metrics table...")
        
        # Prepare corrected data for storage
        corrected_user_metrics = comprehensive_metrics_df.select(
            "user_id",
            "first_login_dt",
            "week_year",
            "user_segment",
            "total_transactions",
            "transaction_days",
            "drop_flag",
            col("return").alias("return_flag"),
            "return_dt",
            "activation_validated",
            "activacion_dt",
            "habit_validated", 
            "total_transactions_period",
            "unique_transaction_days",
            "setup_achieved",
            "setup_dt",
            "onboarding_end_dt",
            "first_login_month",
            "first_login_week"
        )
        
        self.write_to_storage_with_fallback(corrected_user_metrics, "user_metrics")
    
    def run_corrected_loading_pipeline(self):
        """
        Run the CORRECTED complete data loading pipeline
        """
        self.logger.info("ðŸ”„ Starting CORRECTED complete data loading pipeline...")
        
        # Load corrected transformed datasets
        datasets = self.load_corrected_transformed_datasets()
        
        # Load corrected core tables
        self.load_corrected_user_segments_table(datasets["user_segments"])
        self.load_corrected_user_metrics_table(datasets["comprehensive_metrics"])
        
        # Create and load corrected aggregated analysis tables
        corrected_funnel_analysis = self.create_corrected_aggregated_funnel_analysis(datasets["comprehensive_metrics"])
        self.write_to_storage_with_fallback(corrected_funnel_analysis, "funnel_analysis")
        
        corrected_monthly_metrics = self.create_corrected_monthly_metrics(datasets["comprehensive_metrics"])
        self.write_to_storage_with_fallback(corrected_monthly_metrics, "monthly_metrics")
        
        corrected_weekly_metrics = self.create_corrected_weekly_metrics(datasets["comprehensive_metrics"])
        self.write_to_storage_with_fallback(corrected_weekly_metrics, "weekly_metrics")
        
        self.logger.info("âœ… CORRECTED data loading pipeline completed successfully!")
        
        return {
            "user_segments_loaded": True,
            "user_metrics_loaded": True,
            "funnel_analysis_loaded": True,
            "monthly_metrics_loaded": True,
            "weekly_metrics_loaded": True
        }

def main():
    """
    Main function to run the CORRECTED data loading pipeline
    """
    print("="*70)
    print("FINTECH ETL - CORRECTED DATA LOADING")
    print("="*70)
    print("âœ… LOADING: Corrected data with proper baselines")
    print("ðŸ“Š STORING: Realistic metrics and aggregations")
    print("="*70)
    
    # Create Spark session with Cassandra connector
    spark = create_spark_session("Corrected_Data_Loading")
    
    try:
        # Initialize corrected loader
        loader = CorrectedDataLoader(spark)
        
        # Run corrected loading pipeline
        results = loader.run_corrected_loading_pipeline()
        
        print("\n" + "="*70)
        print("âœ… CORRECTED LOADING RESULTS")
        print("="*70)
        
        for table, status in results.items():
            status_text = "âœ… SUCCESS" if status else "âŒ FAILED"
            print(f"{table}: {status_text}")
        
        print("\n" + "="*70)
        print("âœ… CORRECTED DATA LOADING COMPLETED!")
        print("="*70)
        
        return results
        
    except Exception as e:
        print(f"âŒ Error during corrected data loading: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    main()