#!/usr/bin/env python3
"""
Complete ETL Pipeline Runner - CORRECTED VERSION

This script runs the entire ETL pipeline with CORRECTED CSV parsing 
and proper data handling for the Fintech onboarding analytics project.

Pipeline Stages:
1. Data Exploration (optional - for verification)
2. Preprocessing (CORRECTED CSV parsing)
3. Transformation (corrected metrics calculation)
4. Loading (corrected analytics storage)

Usage:
    python run_complete_pipeline.py [--skip-exploration] [--stage STAGE]
"""

import sys
import os
import argparse
import time
from datetime import datetime

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def print_banner(title, width=80):
    """Print a formatted banner"""
    print("=" * width)
    print(f"{title:^{width}}")
    print("=" * width)

def print_stage_header(stage_num, stage_name, description):
    """Print a formatted stage header"""
    print(f"\nğŸ”„ STAGE {stage_num}: {stage_name}")
    print(f"ğŸ“ {description}")
    print("-" * 60)

def run_data_exploration():
    """Run the data exploration stage"""
    print_stage_header(1, "DATA EXPLORATION", "Analyzing raw datasets and identifying issues")
    
    try:
        from notebooks.data_exploration import main as explore_main
        explore_main()
        print("âœ… Data exploration completed successfully")
        return True
    except Exception as e:
        print(f"âŒ Data exploration failed: {e}")
        return False

def run_corrected_preprocessing():
    """Run the corrected preprocessing stage"""
    print_stage_header(2, "CORRECTED PREPROCESSING", "Cleaning data with proper CSV parsing (13,000 users)")
    
    try:
        from src.preprocessing.corrected_data_preprocessing import main as preprocess_main
        preprocess_main()
        print("âœ… Corrected preprocessing completed successfully")
        return True
    except Exception as e:
        print(f"âŒ Corrected preprocessing failed: {e}")
        return False

def run_corrected_transformation():
    """Run the corrected transformation stage"""
    print_stage_header(3, "CORRECTED TRANSFORMATION", "Calculating metrics with proper baselines")
    
    try:
        from src.transformation.corrected_data_transformation import main as transform_main
        transform_main()
        print("âœ… Corrected transformation completed successfully")
        return True
    except Exception as e:
        print(f"âŒ Corrected transformation failed: {e}")
        return False

def run_corrected_loading():
    """Run the corrected loading stage"""
    print_stage_header(4, "CORRECTED LOADING", "Storing analytics with corrected data")
    
    try:
        from src.loading.corrected_data_loading import main as load_main
        load_main()
        print("âœ… Corrected loading completed successfully")
        return True
    except Exception as e:
        print(f"âŒ Corrected loading failed: {e}")
        return False

def run_pipeline_verification():
    """Run pipeline verification and summary"""
    print_stage_header(5, "VERIFICATION", "Verifying pipeline results and generating summary")
    
    try:
        # Check if all expected files exist
        expected_files = [
            "processed_data/clean_users.parquet",
            "processed_data/clean_transactions.parquet", 
            "processed_data/clean_onboarding.parquet",
            "processed_data/user_metrics.parquet",
            "processed_data/final_analytics.parquet"
        ]
        
        missing_files = []
        for file_path in expected_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
        
        if missing_files:
            print(f"âŒ Missing expected files: {missing_files}")
            return False
        
        print("âœ… All expected output files found")
        
        # Print summary statistics
        print("\nğŸ“Š PIPELINE SUMMARY:")
        print("  âœ… CSV parsing issue CORRECTED")
        print("  âœ… Users dataset: ~13,000 actual records (not 39,000 lines)")
        print("  âœ… All business metrics calculated with proper baselines")
        print("  âœ… Analytics data ready for consumption")
        
        return True
        
    except Exception as e:
        print(f"âŒ Pipeline verification failed: {e}")
        return False

def main():
    """Main pipeline runner function"""
    parser = argparse.ArgumentParser(description="Run the complete ETL pipeline with corrections")
    parser.add_argument("--skip-exploration", action="store_true", 
                       help="Skip data exploration stage")
    parser.add_argument("--stage", choices=["exploration", "preprocessing", "transformation", "loading", "verification"],
                       help="Run only a specific stage")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose output")
    
    args = parser.parse_args()
    
    # Print main banner
    print_banner("FINTECH ETL PIPELINE - CORRECTED VERSION")
    print(f"ğŸ•’ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ Working directory: {os.getcwd()}")
    print(f"ğŸ Python executable: {sys.executable}")
    
    # Track overall success
    overall_success = True
    start_time = time.time()
    
    # Define pipeline stages
    stages = []
    
    if not args.skip_exploration and (not args.stage or args.stage == "exploration"):
        stages.append(("exploration", run_data_exploration))
    
    if not args.stage or args.stage == "preprocessing":
        stages.append(("preprocessing", run_corrected_preprocessing))
    
    if not args.stage or args.stage == "transformation":
        stages.append(("transformation", run_corrected_transformation))
    
    if not args.stage or args.stage == "loading":
        stages.append(("loading", run_corrected_loading))
    
    if not args.stage or args.stage == "verification":
        stages.append(("verification", run_pipeline_verification))
    
    # Run stages
    for stage_name, stage_function in stages:
        if args.stage and args.stage != stage_name:
            continue
            
        stage_start = time.time()
        success = stage_function()
        stage_duration = time.time() - stage_start
        
        if success:
            print(f"âœ… {stage_name.upper()} completed in {stage_duration:.1f}s")
        else:
            print(f"âŒ {stage_name.upper()} failed after {stage_duration:.1f}s")
            overall_success = False
            
            # Ask user if they want to continue
            if len(stages) > 1:  # Only ask if there are more stages
                response = input("\nâ“ Continue with remaining stages? (y/n): ")
                if response.lower() != 'y':
                    break
    
    # Print final summary
    total_duration = time.time() - start_time
    print_banner("PIPELINE EXECUTION SUMMARY")
    
    if overall_success:
        print("ğŸ‰ PIPELINE COMPLETED SUCCESSFULLY!")
        print(f"âœ… All stages executed without errors")
        print(f"â±ï¸  Total execution time: {total_duration:.1f} seconds")
        print("\nğŸ“ Output files location: processed_data/")
        print("ğŸ“Š Key corrections applied:")
        print("   - Fixed CSV parsing for multiline address fields")
        print("   - Corrected user count from 39,000 to 13,000")
        print("   - Recalculated all metrics with proper baselines")
        print("   - Generated realistic business analytics")
    else:
        print("âŒ PIPELINE COMPLETED WITH ERRORS")
        print(f"âš ï¸  Some stages failed - check logs above")
        print(f"â±ï¸  Total execution time: {total_duration:.1f} seconds")
        print("\nğŸ”§ Troubleshooting:")
        print("   - Check error messages above")
        print("   - Verify input data files exist")
        print("   - Ensure proper Python environment")
        print("   - Run individual stages for debugging")
    
    print(f"\nğŸ•’ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    return 0 if overall_success else 1

if __name__ == "__main__":
    sys.exit(main())